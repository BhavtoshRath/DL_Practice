{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "\n",
    "\n",
    "### **Activation Functions: What & Why?**  \n",
    "1. **Activation functions** introduce **non-linearity** into a neural network, enabling it to learn **complex patterns**. \n",
    "2. They determine whether a neuron should be **activated** based on its weighted input.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Why Are Activation Functions Important?**  \n",
    "‚úÖ **Introduce non-linearity** ‚Üí Allow neural networks to model complex data.  \n",
    "‚úÖ **Enable deep learning** ‚Üí Without activation functions, multiple layers behave like a **single-layer perceptron** (just a linear transformation, i.e. $Z = WX + b$).  \n",
    "‚úÖ **Help control gradient flow** ‚Üí Prevent vanishing/exploding gradients.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Types of Activation Functions**\n",
    "### **Comparison of Activation Functions**  \n",
    "\n",
    "| **Activation Function** | **Formula** | **Pros ‚úÖ** | **Cons ‚ùå** | **Common Uses** |\n",
    "|------------------|------------------|------------|------------|---------------|\n",
    "| **Linear (Identity Function)** | $ f(x) = x $ | ‚úÖ Used in **regression tasks** (output layer). | ‚ùå **No non-linearity** ‚Üí Cannot learn complex patterns. | Regression models |\n",
    "| **Step Function (Threshold Activation)** | $ f(x) = \\begin{cases} 1, & x \\geq 0 \\\\ 0, & x < 0 \\end{cases} $ | ‚úÖ Used in **Perceptrons** for binary classification. | ‚ùå **Not differentiable**, so **cannot be used in gradient-based learning (e.g., backpropagation).** | Perceptrons (historical use) |\n",
    "| **Sigmoid (Logistic Activation)** | $ f(x) = \\frac{1}{1 + e^{-x}} $ | ‚úÖ **Smooth**, outputs between **0 and 1** (useful for probability). <br> ‚úÖ Used in **binary classification**. | ‚ùå **Vanishing gradient problem** ‚Üí Large inputs cause gradients to be **very small**, slowing learning. | Logistic Regression, Binary Classification |\n",
    "| **Tanh (Hyperbolic Tangent)** | $ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $ | ‚úÖ **Zero-centered** output (-1 to 1), better than Sigmoid. <br> ‚úÖ Used in **RNNs**, where balanced gradients are needed. | ‚ùå Still suffers from the **vanishing gradient problem**. | Recurrent Neural Networks (RNNs) |\n",
    "| **ReLU (Rectified Linear Unit)** | $ f(x) = \\begin{cases} x, & x > 0 \\\\ 0, & x \\leq 0 \\end{cases} $ | ‚úÖ **Efficient** ‚Üí Only requires `max(0, x)`. <br> ‚úÖ **Solves vanishing gradient** (for positive inputs). <br> ‚úÖ Used in **CNNs, Deep Networks**. | ‚ùå **Dying ReLU Problem** ‚Üí Neurons can get stuck at **0** if weights are poorly initialized. | Deep Neural Networks (CNNs, MLPs) |\n",
    "| **Leaky ReLU (Improved ReLU)** | $ f(x) = \\begin{cases} x, & x > 0 \\\\ 0.01x, & x \\leq 0 \\end{cases} $ | ‚úÖ Solves **Dying ReLU** issue by allowing small negative values. <br> ‚úÖ Used in **deep learning architectures**. | ‚ùå Small negative slope is a hyperparameter that needs tuning. | Deep Learning Architectures |\n",
    "| **Softmax (Multi-Class Activation)** | $ f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $ | ‚úÖ Used for **multi-class classification (last layer in classifiers like CNNs).** <br> ‚úÖ Outputs **probabilities that sum to 1**. | ‚ùå Computationally expensive due to exponentials. | Multi-Class Classification (CNNs, NLP Models) |\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ Final Takeaway**\n",
    "- **ReLU** ‚Üí Best for **hidden layers** in deep networks.  \n",
    "- **Softmax** ‚Üí Best for **multi-class classification** (last layer).  \n",
    "- **Sigmoid/Tanh** ‚Üí Used in **binary classification & RNNs**, but may cause **vanishing gradients**.  \n",
    "- **Leaky ReLU** ‚Üí Fixes ReLU‚Äôs **dying neuron problem**.  \n",
    "\n",
    "\n",
    "## **üîπ Choosing the Right Activation Function**\n",
    "| **Use Case** | **Best Activation** |\n",
    "|-------------|------------------|\n",
    "| **Binary Classification** | Sigmoid (last layer) |\n",
    "| **Multi-Class Classification** | Softmax (last layer) |\n",
    "| **Hidden Layers in Deep Networks** | ReLU / Leaky ReLU |\n",
    "| **RNNs (Sequence Models)** | Tanh / Leaky ReLU |\n",
    "| **Regression Output** | Linear (No activation) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Output: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# In numpy\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"Sigmoid Output:\", sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import torch\n",
    "\n",
    "def sigmoid_torch(x):\n",
    "    return 1/ (1 + torch.exp(-x))\n",
    "\n",
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Custom Sigmoid Output:\", sigmoid_torch(x))\n",
    "\n",
    "# Using built-in PyTorch function\n",
    "print(\"Torch Sigmoid Output:\", torch.sigmoid(x)) # torch.sigmoid(x) is optimized for NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Output: [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "# In numpy\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/ (np.sum(np.exp(x)))\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "print(\"Softmax Output:\", softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Output: tensor([0.6590, 0.2424, 0.0986])\n",
      "Inbuilt softmax Output: tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def softmax_torch(x):\n",
    "    return torch.exp(x)/ (torch.sum(torch.exp(x)))\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "print(\"Softmax Output:\", softmax_torch(x))\n",
    "\n",
    "print(\"Inbuilt softmax Output:\", torch.softmax(x, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
