{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "\n",
    "\n",
    "### **Activation Functions: What & Why?**  \n",
    "1. **Activation functions** introduce **non-linearity** into a neural network, enabling it to learn **complex patterns**. \n",
    "2. They determine whether a neuron should be **activated** based on its weighted input.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Why Are Activation Functions Important?**  \n",
    "‚úÖ **Introduce non-linearity** ‚Üí Allow neural networks to model complex data.  \n",
    "‚úÖ **Enable deep learning** ‚Üí Without activation functions, multiple layers behave like a **single-layer perceptron** (just a linear transformation, i.e. $Z = WX + b$).  \n",
    "‚úÖ **Help control gradient flow** ‚Üí Prevent vanishing/exploding gradients.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Types of Activation Functions**\n",
    "### **Comparison of Activation Functions**  \n",
    "\n",
    "| **Activation Function** | **Formula** | **Pros ‚úÖ** | **Cons ‚ùå** | **Common Uses** |\n",
    "|------------------|------------------|------------|------------|---------------|\n",
    "| **Linear (Identity Function)** | $ f(x) = x $ | ‚úÖ Used in **regression tasks** (output layer). | ‚ùå **No non-linearity** ‚Üí Cannot learn complex patterns. | Regression models |\n",
    "| **Step Function (Threshold Activation)** | $ f(x) = \\begin{cases} 1, & x \\geq 0 \\\\ 0, & x < 0 \\end{cases} $ | ‚úÖ Used in **Perceptrons** for binary classification. | ‚ùå **Not differentiable**, so **cannot be used in gradient-based learning (e.g., backpropagation).** | Perceptrons (historical use) |\n",
    "| **Sigmoid (Logistic Activation)** | $ f(x) = \\frac{1}{1 + e^{-x}} $ | ‚úÖ **Smooth**, outputs between **0 and 1** (useful for probability). <br> ‚úÖ Used in **binary classification**. | ‚ùå **Vanishing gradient problem** ‚Üí Large inputs cause gradients to be **very small**, slowing learning. | Logistic Regression, Binary Classification |\n",
    "| **Tanh (Hyperbolic Tangent)** | $ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $ | ‚úÖ **Zero-centered** output (-1 to 1), better than Sigmoid. <br> ‚úÖ Used in **RNNs**, where balanced gradients are needed. | ‚ùå Still suffers from the **vanishing gradient problem**. | Recurrent Neural Networks (RNNs) |\n",
    "| **ReLU (Rectified Linear Unit)** | $ f(x) = \\begin{cases} x, & x > 0 \\\\ 0, & x \\leq 0 \\end{cases} $ | ‚úÖ **Efficient** ‚Üí Only requires `max(0, x)`. <br> ‚úÖ **Solves vanishing gradient** (for positive inputs). <br> ‚úÖ Used in **CNNs, Deep Networks**. | ‚ùå **Dying ReLU Problem** ‚Üí Neurons can get stuck at **0** if weights are poorly initialized. | Deep Neural Networks (CNNs, MLPs) |\n",
    "| **Leaky ReLU (Improved ReLU)** | $ f(x) = \\begin{cases} x, & x > 0 \\\\ 0.01x, & x \\leq 0 \\end{cases} $ | ‚úÖ Solves **Dying ReLU** issue by allowing small negative values. <br> ‚úÖ Used in **deep learning architectures**. | ‚ùå Small negative slope is a hyperparameter that needs tuning. | Deep Learning Architectures |\n",
    "| **Softmax (Multi-Class Activation)** | $ f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $ | ‚úÖ Used for **multi-class classification (last layer in classifiers like CNNs).** <br> ‚úÖ Outputs **probabilities that sum to 1**. | ‚ùå Computationally expensive due to exponentials. | Multi-Class Classification (CNNs, NLP Models) |\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ Final Takeaway**\n",
    "- **ReLU** ‚Üí Best for **hidden layers** in deep networks.  \n",
    "- **Softmax** ‚Üí Best for **multi-class classification** (last layer).  \n",
    "- **Sigmoid/Tanh** ‚Üí Used in **binary classification & RNNs**, but may cause **vanishing gradients**.  \n",
    "- **Leaky ReLU** ‚Üí Fixes ReLU‚Äôs **dying neuron problem**.  \n",
    "\n",
    "\n",
    "## **üîπ Choosing the Right Activation Function**\n",
    "| **Use Case** | **Best Activation** |\n",
    "|-------------|------------------|\n",
    "| **Binary Classification** | Sigmoid (last layer) |\n",
    "| **Multi-Class Classification** | Softmax (last layer) |\n",
    "| **Hidden Layers in Deep Networks** | ReLU / Leaky ReLU |\n",
    "| **RNNs (Sequence Models)** | Tanh / Leaky ReLU |\n",
    "| **Regression Output** | Linear (No activation) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Output: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# In numpy\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"Sigmoid Output:\", sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import torch\n",
    "\n",
    "def sigmoid_torch(x):\n",
    "    return 1/ (1 + torch.exp(-x))\n",
    "\n",
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Custom Sigmoid Output:\", sigmoid_torch(x))\n",
    "\n",
    "# Using built-in PyTorch function\n",
    "print(\"Torch Sigmoid Output:\", torch.sigmoid(x)) # torch.sigmoid(x) is optimized for NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Output: [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "# In numpy\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/ (np.sum(np.exp(x)))\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "print(\"Softmax Output:\", softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Output: tensor([0.6590, 0.2424, 0.0986])\n",
      "Inbuilt softmax Output: tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def softmax_torch(x):\n",
    "    return torch.exp(x)/ (torch.sum(torch.exp(x)))\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "print(\"Softmax Output:\", softmax_torch(x))\n",
    "\n",
    "print(\"Inbuilt softmax Output:\", torch.softmax(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üî• Activation Function Interview Questions (With Answers)**  \n",
    "\n",
    "Here‚Äôs a list of commonly asked **interview questions** on activation functions, ranging from **basic** to **advanced** topics.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Basic Questions**\n",
    "### **1Ô∏è‚É£ What is an activation function in a neural network?**  \n",
    "‚úÖ **Answer:** An **activation function** introduces **non-linearity** to a neural network, enabling it to learn **complex patterns**. It determines whether a neuron should be activated based on its weighted input.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Why do we need activation functions in neural networks?**  \n",
    "‚úÖ **Answer:** Without activation functions, a neural network would **only perform linear transformations** (i.e., matrix multiplications). Activation functions allow the model to learn **non-linear relationships**, making deep learning powerful.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ What are the most commonly used activation functions?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Sigmoid** ‚Üí Used in binary classification.  \n",
    "- **Tanh** ‚Üí Zero-centered version of Sigmoid, used in RNNs.  \n",
    "- **ReLU (Rectified Linear Unit)** ‚Üí Most popular for hidden layers in deep networks.  \n",
    "- **Leaky ReLU** ‚Üí Fixes the **dying ReLU problem**.  \n",
    "- **Softmax** ‚Üí Used in the last layer for multi-class classification.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Intermediate Questions**\n",
    "### **4Ô∏è‚É£ What is the difference between Sigmoid and Softmax?**  \n",
    "‚úÖ **Answer:**  \n",
    "\n",
    "| Feature | **Sigmoid** | **Softmax** |\n",
    "|---------|-----------|------------|\n",
    "| **Output Range** | (0,1) | (0,1), but sums to **1** across classes |\n",
    "| **Use Case** | Binary classification | Multi-class classification |\n",
    "| **Interpretability** | Independent probabilities | Relative probabilities across multiple classes |\n",
    "\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ Why is ReLU preferred over Sigmoid and Tanh in deep networks?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Avoids vanishing gradient problem** (gradients remain large for positive values).  \n",
    "- **Computationally efficient** (only requires `max(0, x)`).  \n",
    "- **Faster convergence** in deep networks.  \n",
    "\n",
    "---\n",
    "\n",
    "### **6Ô∏è‚É£ What is the vanishing gradient problem? Which activation functions suffer from it?**  \n",
    "‚úÖ **Answer:** The **vanishing gradient problem** occurs when gradients become too small during backpropagation, slowing down learning.  \n",
    "üîπ **Sigmoid and Tanh** suffer from this because their gradients approach **0** for large or small values.  \n",
    "üîπ **ReLU does not** suffer from this issue **for positive values** but has the **dying ReLU problem**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **7Ô∏è‚É£ What is the dying ReLU problem? How do we fix it?**  \n",
    "‚úÖ **Answer:**  \n",
    "- If **ReLU outputs 0** for negative inputs, neurons may **stop learning** entirely.  \n",
    "- **Solution:** Use **Leaky ReLU**, which allows a small negative slope (e.g., `f(x) = 0.01x` for \\( x < 0 \\)).  \n",
    "\n",
    "---\n",
    "\n",
    "### **8Ô∏è‚É£ Why is Softmax used in the last layer of multi-class classification?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Softmax ensures outputs sum to 1**, making them interpretable as class probabilities.  \n",
    "- Helps in **argmax-based classification**, choosing the class with the highest probability.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Advanced Questions**\n",
    "### **9Ô∏è‚É£ What happens if we remove activation functions from a neural network?**  \n",
    "‚úÖ **Answer:** The network **collapses into a linear model**, meaning multiple layers **will have no advantage** over a single-layer perceptron. It will fail to learn complex patterns.  \n",
    "\n",
    "---\n",
    "\n",
    "### **üîü What are Swish and GELU activations? Why are they used in modern deep learning models?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **Swish:** \\( f(x) = x \\cdot \\text{sigmoid}(x) \\) (smooth and non-monotonic).  \n",
    "- **GELU (Gaussian Error Linear Unit):** Used in **Transformers (BERT, GPT)** because it **improves training stability and convergence**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£1Ô∏è‚É£ How do activation functions impact training time and model performance?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **ReLU and Leaky ReLU are computationally efficient** (piecewise linear).  \n",
    "- **Sigmoid and Softmax are computationally expensive** due to exponentiation.  \n",
    "- **Choosing the right activation function affects convergence speed and final accuracy.**  \n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£2Ô∏è‚É£ Can we use ReLU in the output layer?**  \n",
    "‚úÖ **Answer:**  \n",
    "- **No**, ReLU is not ideal for outputs because it has **no upper bound**.  \n",
    "- **Better choices:**  \n",
    "  - **Regression tasks** ‚Üí Linear activation (`f(x) = x`).  \n",
    "  - **Binary classification** ‚Üí Sigmoid.  \n",
    "  - **Multi-class classification** ‚Üí Softmax.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üî• Rapid-Fire Concept Checks**\n",
    "‚úî What activation function is best for multi-class classification? ‚Üí **Softmax**  \n",
    "‚úî Which activation function is best for hidden layers? ‚Üí **ReLU / Leaky ReLU**  \n",
    "‚úî What is the key problem with Sigmoid? ‚Üí **Vanishing gradients**  \n",
    "‚úî How does Leaky ReLU fix dying ReLU? ‚Üí **Allows small negative values**  \n",
    "‚úî Why do deep networks need non-linear activation? ‚Üí **To learn complex patterns**  \n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ Final Takeaway**\n",
    "- **ReLU is best for hidden layers**.  \n",
    "- **Softmax is best for multi-class classification**.  \n",
    "- **Leaky ReLU fixes dying ReLU**.  \n",
    "- **Vanishing gradients affect Sigmoid & Tanh**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
