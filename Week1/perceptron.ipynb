{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to implement a Perceptron\n",
    "\n",
    "1. Define a Perceptron model (a single-layer neural network).\n",
    "2. Use forward propagation to compute predictions.\n",
    "3. Compute loss (Binary Cross-Entropy).\n",
    "4. Use gradient descent to update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7898\n",
      "Epoch 10, Loss: 0.6798\n",
      "Epoch 20, Loss: 0.6188\n",
      "Epoch 30, Loss: 0.5808\n",
      "Epoch 40, Loss: 0.5538\n",
      "Epoch 50, Loss: 0.5323\n",
      "Epoch 60, Loss: 0.5139\n",
      "Epoch 70, Loss: 0.4975\n",
      "Epoch 80, Loss: 0.4824\n",
      "Epoch 90, Loss: 0.4685\n",
      "\n",
      "Predictions:\n",
      " tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Define the model\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1) # Single neuron with weights and bias (y = xW + b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# 2. Create dataset\n",
    "X = torch.tensor(\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [1, 1]\n",
    "    ],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "y = torch.tensor(\n",
    "    [[0], [0], [0], [1]],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# 3. Initialize model, loss function, and optimizer\n",
    "model = Perceptron(input_size=2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "# 4. Training loop\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad() # reset gradients of the model's parameters to 0\n",
    "    outputs = model(X) # Forward pass\n",
    "    loss = criterion(outputs, y) # Compute loss\n",
    "    loss.backward() # Backward pass\n",
    "    optimizer.step() # Update weights using gradient descent\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# 55. Test the perceptron\n",
    "with torch.no_grad():\n",
    "    preds = model(X).round()\n",
    "    print(\"\\nPredictions:\\n\", preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interview Questions:**  \n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ“Œ Basic Questions (Conceptual)**\n",
    "\n",
    "1. **What is a Perceptron, and how does it work?**  \n",
    "   - **Answer:** A perceptron is the simplest type of artificial neural network, consisting of a single layer of neurons. It computes a weighted sum of the inputs, applies an activation function (e.g., sigmoid or step function), and outputs a binary classification.  \n",
    "\n",
    "2. **Why do we use the sigmoid activation function in the given Perceptron implementation?**  \n",
    "   - **Answer:** Sigmoid outputs values between **0 and 1**, making it suitable for binary classification. It also allows the model to learn using gradient-based optimization (e.g., SGD).  \n",
    "\n",
    "3. **Can a single-layer Perceptron solve non-linearly separable problems?**  \n",
    "   - **Answer:** No, a **single-layer Perceptron** can only learn **linearly separable** functions (e.g., AND, OR). It **cannot** learn XOR since XOR is **non-linearly separable**.  \n",
    "   Note: Logistic Regression and perceptron can only non-linearly separable problems\n",
    "\n",
    "4. **Why do we use Binary Cross-Entropy (BCELoss) instead of Mean Squared Error (MSE) for classification?**  \n",
    "   - **Answer:** BCELoss is better suited for **probabilistic outputs** because it maximizes the log likelihood, leading to better convergence in classification problems. MSE is mainly for regression and does not perform well with probabilities.  \n",
    "\n",
    "5. **What is the role of `optimizer.zero_grad()` in the training loop?**  \n",
    "   - **Answer:** It resets the **gradients** before backpropagation to prevent accumulation from previous iterations.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ“Œ Advanced Questions (Optimization, Training, and Modifications)**\n",
    "6. **Why do we use `torch.sigmoid()` in the forward function instead of applying it inside the loss function?**  \n",
    "   - **Answer:** In binary classification (when we use `BCELoss()`), it's common to apply `sigmoid()` in the model. However, if we were using `BCEWithLogitsLoss()`, PyTorch would automatically apply `sigmoid()`, making it unnecessary in the forward pass.  \n",
    "\n",
    "7. **What happens if we increase or decrease the learning rate (`lr=0.1`)?**  \n",
    "   - **Answer:**  \n",
    "   - **Too high (`lr > 1`)** â†’ Model may overshoot optimal weights and fail to converge.  \n",
    "   - **Too low (`lr < 0.01`)** â†’ Training becomes slow, taking longer to converge.  \n",
    "\n",
    "8. **How would you modify this Perceptron to handle multi-class classification?**  \n",
    "   - **Answer:**  \n",
    "     - Replace **output layer** (`nn.Linear(input_size, 1)`) with `nn.Linear(input_size, num_classes)`.  \n",
    "     - Use **softmax activation** instead of sigmoid.  \n",
    "     - Use **CrossEntropyLoss** instead of BCELoss.  \n",
    "\n",
    "9. **Why do we use `outputs.round()` during testing?**  \n",
    "   - **Answer:** The model outputs probabilities (values between 0 and 1). `round()` converts them into binary values (`0 or 1`) for classification.  \n",
    "\n",
    "10. **How can we make this model work with GPUs?**  \n",
    "    - **Answer:** Move the model and tensors to CUDA using `model.to(\"cuda\")` and `X.to(\"cuda\")`.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ“Œ Code Debugging and Practical Questions**\n",
    "11. **If the loss does not decrease, what are possible reasons?**  \n",
    "   - **Answer:**  \n",
    "     - **Learning rate is too high/low**.  \n",
    "     - **Data is not linearly separable** (e.g., XOR problem).  \n",
    "     - **Training data is too small or imbalanced**.  \n",
    "     - **Weight initialization is poor** (use Xavier/He initialization).  \n",
    "\n",
    "12. **How can we visualize decision boundaries for this Perceptron?**  \n",
    "   - **Answer:** Plot the decision boundary using `matplotlib` by creating a mesh grid and computing model predictions for each point.  \n",
    "\n",
    "13. **What is the role of the `requires_grad=True` in PyTorch tensors?**  \n",
    "   - **Answer:** It enables **autograd (automatic differentiation)** to compute gradients during backpropagation.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ“Œ System Design & Performance**\n",
    "14. **How would you optimize this perceptron for large datasets?**  \n",
    "   - **Answer:**  \n",
    "     - Use **mini-batch training** instead of full-batch.  \n",
    "     - Use **Adam optimizer** instead of SGD for better convergence.  \n",
    "     - Implement **early stopping** to prevent overfitting.  \n",
    "\n",
    "15. **What are the limitations of a Perceptron, and how can we overcome them?**  \n",
    "   - **Answer:**  \n",
    "     - **Cannot learn non-linearly separable problems** â†’ Use a **multi-layer perceptron (MLP)** with hidden layers.  \n",
    "     - **Slow convergence** â†’ Use **batch normalization** or **adaptive optimizers (Adam, RMSprop)**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
