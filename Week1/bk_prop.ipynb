{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before backpropagation: 0.6931042210040035\n",
      "Updated W1:\n",
      " [[ 0.00496985 -0.00136578  0.00647687]\n",
      " [ 0.01525736 -0.00235334 -0.0023413 ]]\n",
      "Updated b1:\n",
      " [[ 2.70575422e-05  1.68672202e-05 -3.20079660e-12]]\n",
      "Updated W2:\n",
      " [[ 0.01584455  0.00762193]\n",
      " [-0.00469517  0.00542603]\n",
      " [-0.00464699 -0.00464449]]\n",
      "Updated b2:\n",
      " [[ 0.00166646 -0.00166646]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation Functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)  # Gradient of ReLU\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability trick\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Loss Function (Cross-Entropy)\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-9)) / m  # Adding small value to avoid log(0)\n",
    "\n",
    "# One-Hot Encoding for Labels\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "# **Sample Input Data (3 samples, 2 features)**\n",
    "X = np.array([[0.5, 1.2],\n",
    "              [1.0, -0.7],\n",
    "              [-0.3, 0.8]])\n",
    "\n",
    "# **Ground Truth Labels**\n",
    "y = np.array([0, 1, 0])  # Class labels\n",
    "y_one_hot = one_hot_encode(y, num_classes=2)  # Convert labels to one-hot\n",
    "\n",
    "# **Initialize Network Parameters**\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(2, 3) * 0.01  # Weights for Layer 1 (2 -> 3 neurons)\n",
    "b1 = np.zeros((1, 3))              # Bias for Layer 1\n",
    "W2 = np.random.randn(3, 2) * 0.01  # Weights for Layer 2 (3 -> 2 neurons)\n",
    "b2 = np.zeros((1, 2))              # Bias for Layer 2\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# **Forward Propagation**\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "A1 = relu(Z1)\n",
    "Z2 = np.dot(A1, W2) + b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "# **Compute Loss**\n",
    "loss = cross_entropy_loss(A2, y_one_hot)\n",
    "print(\"Loss before backpropagation:\", loss)\n",
    "\n",
    "# **Backward Propagation**\n",
    "m = X.shape[0]  # Number of samples\n",
    "\n",
    "# Compute Gradients for Output Layer\n",
    "dZ2 = A2 - y_one_hot       # Gradient of softmax + cross-entropy\n",
    "dW2 = np.dot(A1.T, dZ2) / m\n",
    "db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "# Compute Gradients for Hidden Layer\n",
    "dA1 = np.dot(dZ2, W2.T)    # Backpropagate error to hidden layer\n",
    "dZ1 = dA1 * relu_derivative(Z1)  # Apply ReLU derivative\n",
    "dW1 = np.dot(X.T, dZ1) / m\n",
    "db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "# **Update Weights using Gradient Descent**\n",
    "W1 -= learning_rate * dW1\n",
    "b1 -= learning_rate * db1\n",
    "W2 -= learning_rate * dW2\n",
    "b2 -= learning_rate * db2\n",
    "\n",
    "# **Print Updated Weights**\n",
    "print(\"Updated W1:\\n\", W1)\n",
    "print(\"Updated b1:\\n\", b1)\n",
    "print(\"Updated W2:\\n\", W2)\n",
    "print(\"Updated b2:\\n\", b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üîπ Backpropagation Interview Questions (With Answers) üöÄ**  \n",
    "\n",
    "#### **Basic Questions**\n",
    "1Ô∏è‚É£ **What is backpropagation in neural networks?**  \n",
    "   **Answer:** Backpropagation is an algorithm used to update model parameters (i.e. train neural networks) by `computing the gradient of the loss function with respect to the model‚Äôs weights`. It propagates the error from the output layer back through the network using the chain rule of calculus.  \n",
    "\n",
    "2Ô∏è‚É£ **Why do we need backpropagation in deep learning?**  \n",
    "   **Answer:** To minimize the loss function (i.e distance between labels and predictions). \n",
    "\n",
    "3Ô∏è‚É£ **What are the key steps in backpropagation?**  \n",
    "   **Answer:**  \n",
    "   - **Forward pass**: Compute activations and output.  \n",
    "   - **Compute loss**: Compare predicted output to actual target.  \n",
    "   - **Backward pass**: Compute gradients of the loss with respect to each weight using the chain rule.  \n",
    "   - **Update weights**: Adjust weights using gradient descent.  \n",
    "\n",
    "4Ô∏è‚É£ **What is the chain rule and how is it used in backpropagation?**  \n",
    "   **Answer:** The chain rule is a fundamental rule in calculus used to compute the derivative of a composition of functions. Backpropagation applies the chain rule to calculate how a small change in weights affects the final output error. \n",
    "   \n",
    "4Ô∏è‚É£ **Why do we use chain rule?** \n",
    "  - The chain rule allows us to compute the derivative of **non-linear activations** (ReLU, Sigmoid, Tanh).  \n",
    "   - Example:  \n",
    "     $\n",
    "     \\frac{dL}{dW} = \\frac{dL}{dA} \\times \\frac{dA}{dZ} \\times \\frac{dZ}{dW}\n",
    "     $\n",
    "     where:  \n",
    "     - $ L $ is loss  \n",
    "     - $ A $ is activation  \n",
    "     - $ Z $ is weighted input  \n",
    "     - $ W $ is weight \n",
    "\n",
    "---\n",
    "\n",
    "#### **Intermediate Questions**\n",
    "\n",
    "6Ô∏è‚É£ **Why is ReLU preferred over sigmoid/tanh in deep networks?**  \n",
    "   **Answer:**  \n",
    "   - **ReLU avoids vanishing gradients** (its derivative is 0 for negative values but 1 for positive values).  \n",
    "   - **Sigmoid and tanh suffer from saturation**, where gradients become too small to update weights effectively.  \n",
    "\n",
    "7Ô∏è‚É£ **What happens if we don‚Äôt use backpropagation correctly?**  \n",
    "   **Answer:**  \n",
    "   - **Incorrect weight updates** ‚Üí Model doesn‚Äôt learn properly.  \n",
    "   - **Exploding/vanishing gradients** ‚Üí Either too large or too small weight updates.  \n",
    "   - **Slow convergence** ‚Üí Training takes too long.  \n",
    "\n",
    "8Ô∏è‚É£ **How does weight initialization impact backpropagation?**  \n",
    "   **Answer:** Poor initialization can cause exploding or vanishing gradients. Popular initialization techniques include:  \n",
    "   - **Xavier Initialization** (for sigmoid/tanh)  \n",
    "   - **He Initialization** (for ReLU)  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Advanced Questions**\n",
    "9Ô∏è‚É£ **What is the vanishing gradient problem? How do we solve it?**  \n",
    "   **Answer:**  \n",
    "   - **Vanishing gradients** occur when derivatives are too small (especially in deep networks), preventing effective learning.  \n",
    "   - **Solutions**:  \n",
    "     - Use **ReLU** instead of sigmoid/tanh.  \n",
    "     - Use **Batch Normalization**.  \n",
    "     - Use **LSTM instead of vanilla RNNs** in sequence models.  \n",
    "\n",
    "üîü **What is the exploding gradient problem? How do we solve it?**  \n",
    "   **Answer:**  \n",
    "   - **Exploding gradients** occur when weight updates become excessively large.  \n",
    "   - **Solutions**:  \n",
    "     - **Gradient Clipping** (limit max gradient value).  \n",
    "     - **Use proper weight initialization (Xavier, He)**.  \n",
    "\n",
    "1Ô∏è‚É£1Ô∏è‚É£ **What is the difference between backpropagation and gradient descent?**  \n",
    "   **Answer:**  \n",
    "   - **Backpropagation** computes gradients using the chain rule.  \n",
    "   - **Gradient descent** updates weights using those gradients.  \n",
    "   - **Backpropagation = Gradient Computation; Gradient Descent = Weight Update**.  \n",
    "\n",
    "1Ô∏è‚É£2Ô∏è‚É£ **How is backpropagation different in CNNs and RNNs?**  \n",
    "   **Answer:**  \n",
    "   - **CNNs**: Gradients are propagated through convolutional layers.  \n",
    "   - **RNNs**: Uses **Backpropagation Through Time (BPTT)**, which considers sequential dependencies.  \n",
    "\n",
    "1Ô∏è‚É£3Ô∏è‚É£ **How does batch size affect backpropagation?**  \n",
    "   **Answer:**  \n",
    "   - **Small batches** ‚Üí Noisy updates, better generalization.  \n",
    "   - **Large batches** ‚Üí More stable updates but can get stuck in sharp minima.  \n",
    "   - **Mini-batch Gradient Descent** (mix of both) is widely used.  \n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Coding-Based Questions**\n",
    "1Ô∏è‚É£4Ô∏è‚É£ **Write a Python function to compute the derivative of ReLU.**  \n",
    "   ```python\n",
    "   def relu_derivative(x):\n",
    "       return (x > 0).astype(float)  # Returns 1 if x > 0, else 0\n",
    "   ```\n",
    "\n",
    "1Ô∏è‚É£5Ô∏è‚É£ **Modify the following NumPy forward propagation code to include backpropagation.**  \n",
    "   (They may give a simple forward propagation code and ask you to implement backpropagation.)  \n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Final Tips for Backpropagation Interviews**\n",
    "‚úÖ **Understand the math**: Derivatives, Chain Rule, Gradient Computation.  \n",
    "‚úÖ **Explain gradient flow**: How gradients pass through different layers.  \n",
    "‚úÖ **Code simple neural networks**: Implement forward and backward propagation in NumPy or PyTorch.  \n",
    "‚úÖ **Know optimization techniques**: Adam, RMSProp, Learning Rate Scheduling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
